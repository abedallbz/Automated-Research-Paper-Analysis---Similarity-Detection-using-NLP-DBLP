{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **importing libraries**"
      ],
      "metadata": {
        "id": "bjjUi7MOnjvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhRU3MYAZpWX",
        "outputId": "0463f910-0f5c-4ea4-b6a9-f64057e6de89"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "nJKd5KzSnHv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5eaf29-83e5-49b8-d5e6-b7a18fd1551c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Reading Data**"
      ],
      "metadata": {
        "id": "acLh49LBoDt-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "x_sk8JyDSewK"
      },
      "outputs": [],
      "source": [
        "import PyPDF2\n",
        "\n",
        "def read_pdf_paper(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        reader = PyPDF2.PdfReader(file)\n",
        "        paper_text = \"\"\n",
        "        for page_number in range(len(reader.pages)):\n",
        "            page = reader.pages[page_number]\n",
        "            paper_text += page.extract_text()\n",
        "    return paper_text\n",
        "\n",
        "paper_file_path = '/content/paper 2.pdf'\n",
        "paper_text = read_pdf_paper(paper_file_path)\n",
        "# print(\"Paper Text:\")\n",
        "# print(paper_text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Citation Extraction **"
      ],
      "metadata": {
        "id": "mUMqcOC1nuGG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_referenced_text(main_paper_text):\n",
        "    referenced_texts = {}\n",
        "\n",
        "    # Regular expression pattern to match reference numbers like [1], [2], etc.\n",
        "    ref_pattern = r'\\[(\\d+)\\][;,.:]?'\n",
        "\n",
        "    # Tokenize text into sentences using NLTK\n",
        "    sentences = main_paper_text.split('. ')  # Split by sentence (assuming sentences end with a period)\n",
        "\n",
        "    # Iterate over the sentences and extract referenced text\n",
        "    for sentence in sentences:\n",
        "        # Find all reference numbers in the sentence\n",
        "        reference_matches = re.findall(ref_pattern, sentence)\n",
        "\n",
        "        if reference_matches:\n",
        "            # Get the last non-empty sentence before the reference number(s)\n",
        "            last_sentence = sentence.strip()\n",
        "            if last_sentence:\n",
        "                # Store referenced text for each reference number found in the sentence\n",
        "                for reference_number in reference_matches:\n",
        "                    if reference_number in referenced_texts:\n",
        "                        referenced_texts[reference_number] += ' ' + last_sentence\n",
        "                    else:\n",
        "                        referenced_texts[reference_number] = last_sentence\n",
        "\n",
        "    return referenced_texts\n"
      ],
      "metadata": {
        "id": "A7BO98GXHx9x"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "referenced_texts = extract_referenced_text(paper_text)\n",
        "print(\"Referenced Texts:\")\n",
        "for ref_number, text in referenced_texts.items():\n",
        "    print(f\"Reference {ref_number}: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QppqyDSjXEJ-",
        "outputId": "c0249a48-7ba6-448b-cf9b-100b8a5975f7"
      },
      "execution_count": 183,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Referenced Texts:\n",
            "Reference 2: 1 Introductio n  \n",
            "The benefits o f combining multiple classifiers b ased on differen t classificatio n \n",
            "methods for the sam e pro blem have bee n assesse d in vari ous fields of pattern \n",
            "recogn ition, inclu ding character reco gnition [1 ], sp eech recognition [2], and  text \n",
            "categorization [3] [2] Denoeux, T\n",
            "Reference 3: 1 Introductio n  \n",
            "The benefits o f combining multiple classifiers b ased on differen t classificatio n \n",
            "methods for the sam e pro blem have bee n assesse d in vari ous fields of pattern \n",
            "recogn ition, inclu ding character reco gnition [1 ], sp eech recognition [2], and  text \n",
            "categorization [3] [3] Yang, Y., Thomas Ault, Thomas Pierce\n",
            "Reference 4: A cl assification p roblem is seen as a  process of \n",
            "infere nces a bout class conce pts from  conc rete exam ples [4] [4] Tin Kam Ho\n",
            "Reference 5: In [5], Se bastiani provides a  state-of-t he-art review on text c ategorization,  \n",
            "including this asp ect, it iden tifies fou r combination functions or rules used for \n",
            "V [5] Sebastiani, F., (2002)\n",
            "Reference 6: MV is the simplest a pproach, where the classification \n",
            "decision on each class is made on the basis of majority classifiers being in favour of \n",
            "that class for a given input [6] Computing the local accuracy for each test document is similar to finding \n",
            "the neighbourhood of the document in a k-nearest neighbour (kNN) method [6] ACC is an intermediate approach between WLC and DCS, where instead of selecting the \n",
            "best classifier with the high est local accuracy, ACC sums all the classifiers together to \n",
            "classify test documents [6] [6] Li, Y\n",
            "Reference 1: [1], was aimed at combining the outputs from  classifiers at the label level, and the \n",
            "prior performance was used indirectly for defining the mass functions, instead of \n",
            "directly for decision making The triplet structure used in this work is shown to give a better result than \n",
            "the dichotomous structure used in [1] References \n",
            "[1] Xu, L., Krzyzak A., Suen, C.Y., (1992)\n",
            "Reference 8: The formal definitions for these functions are given below [8]: \n",
            "Definiti on 1 Let Θ be a  frame of discernment, given a s ubset H ⊆ Θ, a m ass function \n",
            "is defined as a mapping m: 2Θ → [0,1], and  satisfies th e following cond itions: \n",
            "1) m(φ)=0 \n",
            "2)∑Θ⊆=\n",
            "HHm 1)(   \n",
            "Definiti on 2 Let Θ be a frame of discer nment and  m be a m ass funct ion on Θ, the \n",
            "belief of a subset H ⊆ Θ is defined a s  \n",
            "  bel(H) =  (1 ) ∑⊆H BBm)(\n",
            "and satisfies the fo llowing conditions: \n",
            "1) bel(φ) = 0, \n",
            "2) ble(Θ) =1  \n",
            "Whe n H is a singleton, m(H) = bel(H) [8] Guan, J.W\n",
            "Reference 11: In [11], Shi indicated that given two pieces of \n",
            "evide nce e1 and e2, and t wo sets of propositions P1 and P2 (P1∩P2 =φ), a pre-requisite \n",
            "of applying the Dem pster's rule of com bination is depe nded on three m ajor fact s, viz, \n",
            "that e1 and e2 are inde pende nt of each ot her; that they support a  singl e set of the \n",
            "propositions, P1 or P2; and that th e propositions are mutually exclusive [11] Shi, S\n",
            "Reference 10: More details can be found in [10] [10] Bi, Y\n",
            "Reference 13: Ex cept fo r a small \n",
            "fraction  of the articles (4 %), each  article b elongs to ex actly one catego ry [13] In o ur expe riments, we use t he ten cross -validation method t o eval uate \n",
            "effective ness of eac h classifie r gene rated by  the four different classification m ethods  \n",
            "of the SVM [15], kNNM [16], kNN [17] and Rocchio [13], and their various \n",
            "combinations [13] Joachims, T\n",
            "Reference 14: Configuration of ten categories of do cuments ( C1: alt.atheism ; C2:comp.graphics; \n",
            "C3:comp.os.ms-windows.misc; C4: com p.sys.ibm.pc.hardw are; C5: com p.sys.mac.hardw are; C6: \n",
            "comp.windows.x, C7: misc .forsale ; C8: rec.autos; C9: rec.motorcycles;  C10: rec.sport.baseb all) \n",
            "Group C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 Total \n",
            "200 √ √ √ √ √ √ √ √ √ √ 20,000 \n",
            "400 √ √ √ √ √ √ √ √ √ √ 40,000 \n",
            "600 √ √ √ √ √ √ √ √ √ √ 60,000 \n",
            "800 √ √ √ √ √ √ √ √ √ √ 80,000 \n",
            "1000 √ √ √ √ √ √ √ √ √ √ 100,000 Combining Multiple Classifiers Using Demp ster’s Rule of  Combination       135 \n",
            "5.2 Perfor mance Analysis \n",
            "We u se information gain as a measure for feature selec tion at pre-proces sing stage for \n",
            "each classifier, and this stage also invol ves the task of rem oving function words  \n",
            "before wei ghting t erms by  using tfidf (term freque ncy and inverse docum ent \n",
            "frequency) [14] [14] Salton, G., Allan, J., Buckley, C\n",
            "Reference 15: In o ur expe riments, we use t he ten cross -validation method t o eval uate \n",
            "effective ness of eac h classifie r gene rated by  the four different classification m ethods  \n",
            "of the SVM [15], kNNM [16], kNN [17] and Rocchio [13], and their various \n",
            "combinations [15] Chang, C\n",
            "Reference 16: In o ur expe riments, we use t he ten cross -validation method t o eval uate \n",
            "effective ness of eac h classifie r gene rated by  the four different classification m ethods  \n",
            "of the SVM [15], kNNM [16], kNN [17] and Rocchio [13], and their various \n",
            "combinations [16] Guo, G., Wang, H., Bell, D., Bi, Y\n",
            "Reference 17: In o ur expe riments, we use t he ten cross -validation method t o eval uate \n",
            "effective ness of eac h classifie r gene rated by  the four different classification m ethods  \n",
            "of the SVM [15], kNNM [16], kNN [17] and Rocchio [13], and their various \n",
            "combinations [17] Yang, Y\n",
            "Reference 7: [7] Larkey, L.S\n",
            "Reference 9: [9] Bi, Y., Bell, D, Guan, J.W\n",
            "Reference 12: [12] van Rijsbergen, C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iJMccW8YXKGr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mylist=[]\n",
        "for ref_number, text in referenced_texts.items():\n",
        "\n",
        "    print(f\"Reference {ref_number}\")\n",
        "    mylist.append(ref_number)\n",
        "len(mylist)"
      ],
      "metadata": {
        "id": "97xb4d2rWrI8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc7326e-bdb1-4ae2-a235-e0fca3a67ae4"
      },
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference 2\n",
            "Reference 3\n",
            "Reference 4\n",
            "Reference 5\n",
            "Reference 6\n",
            "Reference 1\n",
            "Reference 8\n",
            "Reference 11\n",
            "Reference 10\n",
            "Reference 13\n",
            "Reference 14\n",
            "Reference 15\n",
            "Reference 16\n",
            "Reference 17\n",
            "Reference 7\n",
            "Reference 9\n",
            "Reference 12\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17"
            ]
          },
          "metadata": {},
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " import re\n",
        "\n",
        "def extract_text_from_sections(reference_paper_text):\n",
        "    # Regular expression patterns to match abstract and conclusion sections\n",
        "    abstract_pattern = r'(?:Abstract\\.|ABSTRACT\\.|ABSTRACT|Abstract)(.*?)(?=INTRODUCTION|\\n\\n)'\n",
        "    conclusion_pattern = r'CONCLUSION\\s*(.*?)(?=REFERENCES|ACKNOWLEDGMENT|\\Z|Future work)'\n",
        "\n",
        "    # Extract abstract and conclusion sections from the reference paper text\n",
        "    abstract_matches = re.findall(abstract_pattern, reference_paper_text, re.IGNORECASE | re.DOTALL)\n",
        "    conclusion_matches = re.findall(conclusion_pattern, reference_paper_text, re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    # Initialize variable to store extracted text\n",
        "    extracted_text = \"\"\n",
        "\n",
        "    # Extract text from abstract section\n",
        "    for abstract_match in abstract_matches:\n",
        "        extracted_text += abstract_match.strip() + \"\\n\"\n",
        "\n",
        "    # Extract text from conclusion section, stop if 'Future work' is encountered\n",
        "    for conclusion_match in conclusion_matches:\n",
        "        if 'Future work' in conclusion_match:\n",
        "            break\n",
        "        extracted_text += conclusion_match.strip()\n",
        "\n",
        "    return extracted_text\n",
        "\n",
        "# Example usage:\n",
        "text_extracted = extract_text_from_sections(paper_text)\n",
        "print(\"Text Extracted:\")\n",
        "print(text_extracted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbL_PdFNbP3H",
        "outputId": "b0ecb6bf-f01b-4679-a09d-79b892e3cdb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text Extracted:\n",
            "Machine learning for text classification is the cornerstone of document categorization, news \n",
            "filtering, document routing, and personalization.  In text domains, effective feature selection is essential to make the learning task efficient and more accurate.  This paper presents an empirical \n",
            "comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark \n",
            "of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc.  The results are analyzed from multiple goal perspectives—accuracy, F-measure, precision, and recall—since each is appropriate in different situations.  The results reveal that a new feature selection metric we call ‘Bi-Normal Separation’ (BNS), outperformed the others by a substantial margin in most situations.  This margin widened in tasks \n",
            "with high class skew, which is rampant in text classification problems and is particularly \n",
            "challenging for induction algorithms.  A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely  to yield the best performance.  From this perspective, BNS was the top single choice for all \n",
            "goals except precision, for which Information Gain yielded the best result most often.  This \n",
            "analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together.  When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair—e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin. \n",
            "Keywords:  support vector machines, document categorization, ROC, supervised learning \n",
            "1\n",
            "This paper presented an extensive comparative study of feature selection metrics for the high-dimensional domain of text classification, focusing on support vector machines and 2-class \n",
            "problems, typically with high class skew. It revealed the surprising performance of a new feature \n",
            "selection metric, Bi-Normal Separation.  \n",
            "Another contribution of this paper is a novel evaluation methodology that considers the \n",
            "common problem of trying to select one or two metrics that have the best chances of obtaining \n",
            "the best performance for a given  dataset. Somewhat surprisingly, selecting the two best-\n",
            "performing metrics can be sub-optimal: when the best metric fails, the other may have correlated failures, as is the case for IG and Chi for maximizing precision.  The residual analysis determined \n",
            "that BNS paired with Odds Ratio yielded the best chances of attaining the best precision.  For \n",
            "optimizing recall, BNS paired with F1 was consistently the best pair by a wide margin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JtVt2H6YGmX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "l8MyFkt7n3r4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def clean_text(text):\n",
        "    # Remove special characters, punctuation, and non-alphanumeric symbols\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    words = nltk.word_tokenize(cleaned_text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "    # Join the words back into a string\n",
        "    cleaned_text = ' '.join(filtered_words)\n",
        "    return cleaned_text\n",
        "\n",
        "# Clean the extracted text\n",
        "# print(clean_text(referenced_texts['2'])\n",
        "# , clean_text(text_extracted))\n",
        "\n",
        "# # Print the cleaned extracted text\n",
        "# print(\"Cleaned Text Extracted:\")\n",
        "# print(cleaned_text_extracted)\n"
      ],
      "metadata": {
        "id": "rs-4JeogBJHp"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1SRmKwWpnhxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize the cleaned text\n",
        "tokenized_text_extracted = nltk.word_tokenize(cleaned_text_extracted)\n",
        "\n",
        "# Print the tokenized text\n",
        "print(\"Tokenized Text Extracted:\")\n",
        "print(tokenized_text_extracted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKLt2E6xbeAJ",
        "outputId": "2cd84ce7-fc99-4220-f9d8-34755f1791c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized Text Extracted:\n",
            "['Automated', 'text', 'classification', 'considered', 'vital', 'method', 'manage', 'process', 'vast', 'amount', 'documents', 'digital', 'forms', 'widesp', 'read', 'continuously', 'incr', 'easing', 'general', 'text', 'classification', 'plays', 'important', 'role', 'information', 'ex', 'traction', 'summarization', 'te', 'xt', 'retrieval', 'question', 'answering', 'paper', 'illustrates', 'text', 'classification', 'process', 'using', 'machine', 'learning', 'techniques', 'references', 'cited', 'cover', 'major', 'theoretical', 'issues', 'gui', 'de', 'researcher', 'interesting', 'research', 'directions', 'KeyWords', 'text', 'mining', 'learning', 'algorithms', 'feature', 'selection', 'text', 'representation', '1', 'text', 'classification', 'problem', 'Artificial', 'Intelligence', 'research', 'topic', 'especially', 'given', 'vast', 'number', 'documents', 'available', 'form', 'web', 'pages', 'electronic', 'texts', 'like', 'emails', 'discussion', 'forum', 'postings', 'electronic', 'documents', 'observed', 'even', 'specified', 'classification', 'method', 'classification', 'performances', 'classifiers', 'based', 'different', 'training', 'text', 'corpuses', 'different', 'cases', 'differences', 'quite', 'substantial', 'observation', 'implies', 'classifier', 'performance', 'relevant', 'training', 'corpus', 'degree', 'b', 'good', 'high', 'quality', 'training', 'corpuses', 'may', 'derive', 'classifiers', 'good', 'performance', 'Unfortunately', 'little', 'research', 'work', 'literature', 'seen', 'exploit', 'training', 'text', 'corpuses', 'improve', 'classifiers', 'performance', 'important', 'conclusions', 'reached', 'yet', 'including', 'feature', 'selection', 'methods', 'computationally', 'scalable', 'highperforming', 'across', 'classifiers', 'collections', 'Given', 'high', 'variability', 'text', 'collections', 'methods', 'even', 'exist', 'Would', 'combining', 'uncorrelated', 'wellperforming', 'methods', 'yield', 'performance', 'increase', 'Change', 'thinking', 'word', 'frequency', 'based', 'vector', 'space', 'concepts', 'based', 'vector', 'space', 'Study', 'methodology', 'feature', 'selection', 'concepts', 'see', 'help', 'text', 'categorization', 'Make', 'dimensionality', 'reduction', 'efficient', 'large', 'corpus', 'Moreover', 'two', 'open', 'problems', 'text', 'mining', 'polysemy', 'synonymy', 'Polysemy', 'refers', 'fact', 'word', 'multiple', 'meanings', 'Distinguishing', 'different', 'meanings', 'word', 'called', 'word', 'sense', 'disambiguation', 'easy', 'often', 'requiring', 'context', 'word', 'appears', 'Synonymy', 'means', 'different', 'words', 'similar', 'meaning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the tokenized text\n",
        "tfidf_vectors = tfidf_vectorizer.fit_transform(tokenized_text_extracted)\n",
        "print(tfidf_vectors)\n",
        "# Print the shape of the TF-IDF vectors\n",
        "print(\"Shape of TF-IDF Vectors:\", tfidf_vectors.shape)\n"
      ],
      "metadata": {
        "id": "T1lTNMdRb0Sm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "id": "oHOs94oDcGuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# method cosine_similarity to find [simliarity](https://)"
      ],
      "metadata": {
        "id": "3rSgxqYUpHKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cosine_similarity(text1, text2):\n",
        "    # Convert tokenized text lists back to strings\n",
        "    text1_str = ' '.join(text1)\n",
        "    text2_str = ' '.join(text2)\n",
        "\n",
        "    # Store text in a list\n",
        "    list_text = [text1_str, text2_str]\n",
        "\n",
        "    # Initialize the TF-IDF vectorizer\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "    # Fit and transform the text\n",
        "    tfidf_vectors = vectorizer.fit_transform(list_text)\n",
        "\n",
        "    # Extract the TF-IDF vectors for the two texts\n",
        "    tfidf_text1, tfidf_text2 = tfidf_vectors[0], tfidf_vectors[1]\n",
        "\n",
        "    # Compute the cosine similarity\n",
        "    cosine_sim = cosine_similarity(tfidf_text1, tfidf_text2)\n",
        "\n",
        "    return cosine_sim[0][0]\n",
        "\n"
      ],
      "metadata": {
        "id": "5ywrYx7bHFX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4OMs7DCa1C1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_similarity(text1, text2):\n",
        "    # Clean the extracted text\n",
        "    cleaned_text1 = clean_text(text1)\n",
        "    cleaned_text2 = clean_text(text2)\n",
        "\n",
        "    # Tokenize the cleaned text\n",
        "    tokenized_text1 = nltk.word_tokenize(cleaned_text1)\n",
        "    tokenized_text2 = nltk.word_tokenize(cleaned_text2)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    similarity_score = compute_cosine_similarity(tokenized_text1, tokenized_text2)\n",
        "\n",
        "    return similarity_score\n"
      ],
      "metadata": {
        "id": "j-pQLtMdHXsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_score = process_text_similarity(text_extracted,referenced_texts['6'])\n",
        "print(\"Cosine Similarity Score:\", similarity_score)\n"
      ],
      "metadata": {
        "id": "RtUw4WzmHeeP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c8e8af-827d-40b4-8652-36f997dd8aee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity Score: 0.1459713412633019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# method 2 word2vector embading method"
      ],
      "metadata": {
        "id": "ySMnGFexvimC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ],
      "metadata": {
        "id": "iGgBZ_TzwAhW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "869e7f0f-c488-494e-b82a-e027a1136ae0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.5.16)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.2)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.0.7)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d pkugoodspeed/nlpword2vecembeddingspretrained"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32tzlo3yvXal",
        "outputId": "eb60df12-c608-49ec-e843-0b395ca8b639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nlpword2vecembeddingspretrained.zip to /content\n",
            "100% 2.46G/2.46G [00:41<00:00, 35.1MB/s]\n",
            "100% 2.46G/2.46G [00:41<00:00, 63.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip nlpword2vecembeddingspretrained.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANUYpWnMwSi4",
        "outputId": "3a0be448-dbdf-4b8a-9ded-cc72d41e809f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  nlpword2vecembeddingspretrained.zip\n",
            "  inflating: GoogleNews-vectors-negative300.bin  \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "  inflating: glove.6B.50d.txt        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # using pretrained model\n",
        "import gensim\n",
        "model = gensim.models.KeyedVectors.load_word2vec_format('//content/GoogleNews-vectors-negative300.bin', binary=True)"
      ],
      "metadata": {
        "id": "Fo81I0ydyEBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def compute_sif_similarity(text1, text2, word_embeddings_model):\n",
        "    # Tokenize the texts\n",
        "    tokens_text1 = text1.split()\n",
        "    tokens_text2 = text2.split()\n",
        "\n",
        "    # Create TF-IDF vectors\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_vectors = vectorizer.fit_transform([text1, text2])\n",
        "\n",
        "    # Get word embeddings for all words in the vocabulary\n",
        "    word_embeddings = {}\n",
        "    for word in vectorizer.vocabulary_.keys():\n",
        "        word_embeddings[word] = word_embeddings_model[word]\n",
        "\n",
        "    # Compute the weighted average of word embeddings for each text\n",
        "    embedding_dim = len(word_embeddings_model.vectors[0])\n",
        "    embeddings_text1 = np.zeros(embedding_dim)\n",
        "    embeddings_text2 = np.zeros(embedding_dim)\n",
        "    total_weight_text1 = 0\n",
        "    total_weight_text2 = 0\n",
        "    for word, index in vectorizer.vocabulary_.items():\n",
        "        tfidf_weight_text1 = tfidf_vectors[0, index]\n",
        "        tfidf_weight_text2 = tfidf_vectors[1, index]\n",
        "        if word in word_embeddings:\n",
        "            embeddings_text1 += tfidf_weight_text1 * word_embeddings[word]\n",
        "            embeddings_text2 += tfidf_weight_text2 * word_embeddings[word]\n",
        "            total_weight_text1 += tfidf_weight_text1\n",
        "            total_weight_text2 += tfidf_weight_text2\n",
        "\n",
        "    # Normalize the embeddings\n",
        "    embeddings_text1 /= max(total_weight_text1, 1)\n",
        "    embeddings_text2 /= max(total_weight_text2, 1)\n",
        "\n",
        "    # Compute cosine similarity between the normalized embeddings\n",
        "    similarity_score = cosine_similarity([embeddings_text1], [embeddings_text2])[0][0]\n",
        "\n",
        "    return similarity_score\n",
        "\n"
      ],
      "metadata": {
        "id": "WXrq94_Huf71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, model_vocab):\n",
        "    # Remove special characters, punctuation, and non-alphanumeric symbols\n",
        "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    # Tokenize the text\n",
        "    tokens = cleaned_text.split()\n",
        "    # Filter out words not in the model's vocabulary\n",
        "    tokens = [token for token in tokens if token in model_vocab]\n",
        "    return tokens\n",
        "\n",
        "# Get the vocabulary of the word embeddings model\n",
        "model_vocab = model.key_to_index.keys()\n",
        "\n",
        "cleaned_tokens_text1 = preprocess_text( abstract_from_url,model_vocab)\n",
        "cleaned_tokens_text2 = preprocess_text(referenced_texts['6'], model_vocab)\n",
        "\n",
        "# Compute similarity using preprocessed tokens\n",
        "similarity_score = compute_sif_similarity(' '.join(cleaned_tokens_text1), ' '.join(cleaned_tokens_text2), model)\n",
        "print(\"Smooth Inverse Frequency Similarity Score:\", similarity_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF8yO1-s6P_l",
        "outputId": "73b4f249-99f4-4d6a-9cbd-ec47b13afaae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Smooth Inverse Frequency Similarity Score: 0.8817568263545884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_similarity(similarity_score,  similarity_threshold=0.7):\n",
        "\n",
        "    if similarity_score > similarity_threshold:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "is_similar = check_similarity(similarity_score)\n",
        "if is_similar:\n",
        "    print(\"The referenced paper and the corresponding part in the main paper are similar.\")\n",
        "else:\n",
        "    print(\"The referenced paper and the corresponding part in the main paper are not similar.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9XDd3UHUuah",
        "outputId": "26a391dc-c340-49bf-c036-e230ceee56ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The referenced paper and the corresponding part in the main paper are similar.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **finding abstract for papers using dblp api**"
      ],
      "metadata": {
        "id": "y8tdatT7e8Vk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using titles to find the *abstract*"
      ],
      "metadata": {
        "id": "d09iRK2sfOIO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6hJqpN1pLJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def search_dblp(title):\n",
        "    base_url = \"https://dblp.org/search/publ/api\"\n",
        "    params = {\n",
        "        \"q\": title,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data\n",
        "\n",
        "def get_abstract_from_url(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the webpage\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find the element containing the abstract\n",
        "        abstract_tag = soup.find('div', class_='abstractSection abstractInFull')\n",
        "        # Extract the text of the abstract if found\n",
        "        if abstract_tag:\n",
        "            abstract = abstract_tag.get_text(strip=True)\n",
        "            return abstract\n",
        "    # Return None if abstract is not found or if there's an error\n",
        "    return None\n",
        "\n",
        "# Example usage:\n",
        "title = \"Integrating Feature and Instance Selection for Text Classification\"\n",
        "result = search_dblp(title)\n",
        "\n",
        "# Check if the paper is found and print its full name and abstract\n",
        "if 'hit' in result['result']['hits']:\n",
        "    first_hit = result['result']['hits']['hit'][0]\n",
        "    paper_info = first_hit['info']\n",
        "    paper_title = paper_info['title']\n",
        "    # paper_abstract = paper_info.get('abstract', 'N/A')\n",
        "    paper_ee = paper_info.get('ee')\n",
        "    print(\"Full Name of the Paper:\", paper_title)\n",
        "    # print(\"Abstract:\", paper_abstract)\n",
        "    if paper_ee:\n",
        "\n",
        "        abstract_from_url = get_abstract_from_url(paper_ee)\n",
        "        print(\"Abstract from URL:\", abstract_from_url)\n",
        "    else:\n",
        "        print(\"No URL found to retrieve abstract.\")\n",
        "else:\n",
        "    print(\"Paper not found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H37rj_FBPrG0",
        "outputId": "e7318790-03ed-42cd-f6e4-73b381353a3a"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full Name of the Paper: Integrating feature and instance selection for text classification.\n",
            "Abstract from URL: Instance selection and feature selection are two orthogonal methods for reducing the amount and complexity of data. Feature selection aims at the reduction of redundant features in a dataset whereas instance selection aims at the reduction of the number of instances. So far, these two methods have mostly been considered in isolation. In this paper, we present a new algorithm, which we callFIS(Feature and Instance Selection) that targets both problems simultaneously in the context of text classificationOur experiments on the Reuters and 20-Newsgroups datasets show that FIS considerably reduces both the number of features and the number of instances. The accuracy of a range of classifiers including Naïve Bayes, TAN and LB considerably improves when using the FIS preprocessed datasets, matching and exceeding that of Support Vector Machines, which is currently considered to be one of the best text classification methods. In all cases the results are much better compared to Mutual Information based feature selection. The training and classification speed of all classifiers is also greatly improved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **finding abstract for all refrences in the main papers automtcally**"
      ],
      "metadata": {
        "id": "8iqupRE8facC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "claen the paper text"
      ],
      "metadata": {
        "id": "V66p8P-7fxTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove extra spaces and line breaks\n",
        "paper_text_cleaned = ' '.join(paper_text.split())\n",
        "\n",
        "# Ensure consistent citation style (e.g., ensure each reference starts with a new line)\n",
        "paper_text_cleaned = paper_text_cleaned.replace(\"[\", \"\\n[\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sXzEMXzDf1dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def search_dblp(title):\n",
        "    base_url = \"https://dblp.org/search/publ/api\"\n",
        "    params = {\n",
        "        \"q\": title,\n",
        "        \"format\": \"json\"\n",
        "    }\n",
        "    response = requests.get(base_url, params=params)\n",
        "    data = response.json()\n",
        "    return data\n",
        "\n",
        "def get_abstract_from_url(url):\n",
        "    # Send a GET request to the URL\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        # Parse the HTML content of the webpage\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        # Find the element containing the abstract\n",
        "        abstract_tag = soup.find('div', class_='abstractSection abstractInFull')\n",
        "        # Extract the text of the abstract if found\n",
        "        if abstract_tag:\n",
        "            abstract = abstract_tag.get_text(strip=True)\n",
        "            return abstract\n",
        "    # Return None if abstract is not found or if there's an error\n",
        "    return None\n",
        "\n",
        "def extract_references(paper_text):\n",
        "    # Regular expression pattern to match reference sections\n",
        "    ref_pattern = r'References(.*?)(?=\\n\\n|$)'\n",
        "    # Find the reference section\n",
        "    references_match = re.search(ref_pattern, paper_text, re.DOTALL)\n",
        "    if references_match:\n",
        "        references_text = references_match.group(1)\n",
        "        # Extract titles and return as a list\n",
        "        return re.findall(r'\\(.+?\\)\\. (.+?)\\.', references_text)\n",
        "    else:\n",
        "        return []\n",
        "\n",
        "# Example usage:\n",
        "\n",
        "references = extract_references(paper_text_cleaned)\n",
        "abstracts = {}\n",
        "for reference, ref_number in zip(references, range(1, len(references) + 1)):\n",
        "    print(\"Reference Number:\", reference)\n",
        "    print(\"Refrence number\", ref_number)\n",
        "\n",
        "    result = search_dblp(reference)\n",
        "    if 'hit' in result['result']['hits']:\n",
        "        first_hit = result['result']['hits']['hit'][0]\n",
        "        paper_info = first_hit['info']\n",
        "        paper_ee = paper_info.get('ee')\n",
        "        if paper_ee:\n",
        "            abstract_from_url = get_abstract_from_url(paper_ee)\n",
        "            abstracts[ref_number] = abstract_from_url\n",
        "            print(\"Abstract from URL:\", abstract_from_url)\n",
        "        else:\n",
        "            print(\"No URL found to retrieve abstract.\")\n",
        "    else:\n",
        "        print(\"Paper not found.\")\n",
        "\n",
        "# Now abstracts dictionary will contain abstracts for each reference number\n",
        "print(abstracts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lM_fjKA-zkA",
        "outputId": "737a3cef-7cc7-44dc-c472-b813fdb7931e"
      },
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reference Number: Several Methods for Combining Multiple Classifiers and Their Applications in Handw ritten Character Recognition\n",
            "Refrence number 1\n",
            "Paper not found.\n",
            "Reference Number: A neural network classi fier based on Dempster-Shafer theory\n",
            "Refrence number 2\n",
            "Paper not found.\n",
            "Reference Number: Combining multiple learning strategies for effective cross validation\n",
            "Refrence number 3\n",
            "No URL found to retrieve abstract.\n",
            "Reference Number: Multiple Classifier Co mbination: Lessons and Next Steps, Tin Kam Ho, in A\n",
            "Refrence number 4\n",
            "Paper not found.\n",
            "Reference Number: Machine Learning in Automated Text Categorization\n",
            "Refrence number 5\n",
            "Abstract from URL: The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.\n",
            "Reference Number: Classifi cation of Text Documents\n",
            "Refrence number 6\n",
            "Paper not found.\n",
            "Reference Number: Combining classifiers in text categorization\n",
            "Refrence number 7\n",
            "Abstract from URL: None\n",
            "Reference Number: Evidence Th eory and its applications, Vol\n",
            "Refrence number 8\n",
            "Paper not found.\n",
            "Reference Number: Combin ing Evidence from Classifiers in Text Categorization\n",
            "Refrence number 9\n",
            "Paper not found.\n",
            "Reference Number: Combining Multiple Classifier s for Text Categorization using Dempster- Shafer Theory of Evidence\n",
            "Refrence number 10\n",
            "Paper not found.\n",
            "Reference Number: On Reasoning with Uncer tainty and Belief Change\n",
            "Refrence number 11\n",
            "Paper not found.\n",
            "Reference Number: Information Retrieval (second edition)\n",
            "Refrence number 12\n",
            "Abstract from URL: None\n",
            "Reference Number: A probabi listic analysis of the Rocch io algorithm with TFIDF for text categorization\n",
            "Refrence number 13\n",
            "Paper not found.\n",
            "Reference Number: A\n",
            "Refrence number 14\n",
            "Abstract from URL: None\n",
            "Reference Number: LIBSVM: a library for support vector machines (http://www\n",
            "Refrence number 15\n",
            "Paper not found.\n",
            "Reference Number: kNN model-based approach in classification\n",
            "Refrence number 16\n",
            "Abstract from URL: None\n",
            "Reference Number: A study on thre sholding strategies for text categorization\n",
            "Refrence number 17\n",
            "Paper not found.\n",
            "{5: 'The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.', 7: None, 12: None, 14: None, 16: None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abstracts[5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0GffE5IWc6T",
        "outputId": "20ea9340-4608-4c67-cc76-1ff937e2cfde"
      },
      "execution_count": 196,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GGDaQDoXlUQD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}